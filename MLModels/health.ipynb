{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829eed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (78.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kiran\\onedrive\\desktop\\pto\\.v\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f03ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import glob\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from difflib import get_close_matches\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Shows plots in jupyter notebook\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Set plot style\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1388941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 CSV files in the current directory.\n",
      "\n",
      "CSV Files Summary:\n",
      "File Name                 Rows       Columns    Size (KB)      \n",
      "------------------------------------------------------------\n",
      "\n",
      "Detailed information for each file:\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os  # Ensure os is imported\n",
    "\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "csv_files.sort()  # Sort alphabetically\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files in the current directory.\\n\")\n",
    "\n",
    "# Create a summary of all files\n",
    "print(\"CSV Files Summary:\")\n",
    "print(f\"{'File Name':<25} {'Rows':<10} {'Columns':<10} {'Size (KB)':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        file_size = os.path.getsize(file) / 1024\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"{file:<25} {df.shape[0]:<10} {df.shape[1]:<10} {file_size:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{file:<25} Error: {str(e)}\")\n",
    "\n",
    "print(\"\\nDetailed information for each file:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read each CSV file and display basic info\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        print(f\"\\nFile: {file}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Read the file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Display basic info\n",
    "        print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "        print(\"\\nColumns:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  - {col}\")\n",
    "        \n",
    "        # Display first 3 rows\n",
    "        print(\"\\nFirst 3 rows:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {str(e)}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e61d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "\n",
    "# Print info for each CSV file\n",
    "for file in csv_files:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FILE: {file}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(\"\\nInfo:\")\n",
    "        df.info()\n",
    "        print(\"\\nSample (first 3 rows):\")\n",
    "        print(df.head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d9f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CSV files in the current directory\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "\n",
    "# Print describe for each CSV file\n",
    "for file in csv_files:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FILE: {file}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        \n",
    "        # Get numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        if numeric_cols:\n",
    "            print(\"\\nNumeric Columns Statistics:\")\n",
    "            print(df[numeric_cols].describe())\n",
    "        else:\n",
    "            print(\"\\nNo numeric columns to describe.\")\n",
    "            \n",
    "        # Get categorical columns (limit to those with fewer than 20 unique values)\n",
    "        cat_cols = [col for col in df.columns if col not in numeric_cols \n",
    "                   and df[col].nunique() < 20 and df[col].nunique() > 0]\n",
    "        \n",
    "        if cat_cols:\n",
    "            print(\"\\nCategorical Columns (Top 5 value counts):\")\n",
    "            for col in cat_cols[:5]:  # Limit to first 5 categorical columns\n",
    "                print(f\"\\n{col}:\")\n",
    "                print(df[col].value_counts().head())\n",
    "        else:\n",
    "            print(\"\\nNo suitable categorical columns to describe.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a876aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV files...\n",
      "\n",
      "Creating entity resolution table...\n",
      "\n",
      "Entity Resolution Table:\n",
      "Empty DataFrame\n",
      "Columns: [in_all_files]\n",
      "Index: []\n",
      "\n",
      "All diseases appear in all files.\n",
      "\n",
      "No similar disease names found.\n",
      "\n",
      "Standardizing disease names...\n",
      "\n",
      "Creating comprehensive table...\n",
      "\n",
      "Saving standardized files...\n",
      "\n",
      "Saving comprehensive table...\n",
      "Saved comprehensive table to output\\comprehensive_disease_data.csv\n",
      "\n",
      "Entity resolution process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def load_csv_files():\n",
    "    \"\"\"Load all CSV files in the current directory into a dictionary of DataFrames.\"\"\"\n",
    "    # Get all CSV files in the current directory\n",
    "    csv_files = glob.glob('*.csv')\n",
    "    \n",
    "    # Create a dictionary to store all dataframes\n",
    "    dfs = {}\n",
    "    \n",
    "    # Read each CSV file\n",
    "    for file in csv_files:\n",
    "        # Get the name without extension\n",
    "        name = file.split('.')[0]\n",
    "        # Read the CSV file\n",
    "        dfs[name] = pd.read_csv(file)\n",
    "        print(f'Read {file} successfully')\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def create_entity_resolution_table(dfs):\n",
    "    \"\"\"Create an entity resolution table for diseases across all files.\"\"\"\n",
    "    # Get unique diseases from each file\n",
    "    diseases = {}\n",
    "    for name, df in dfs.items():\n",
    "        if 'Disease' in df.columns:\n",
    "            diseases[name] = set(df['Disease'].unique())\n",
    "    \n",
    "    # Get all unique diseases across all files\n",
    "    all_diseases = set()\n",
    "    for disease_set in diseases.values():\n",
    "        all_diseases.update(disease_set)\n",
    "    \n",
    "    # Create a DataFrame to track which diseases appear in which files\n",
    "    entity_df = pd.DataFrame(index=sorted(all_diseases))\n",
    "    \n",
    "    # Fill the DataFrame with True/False values indicating presence in each file\n",
    "    for name, disease_set in diseases.items():\n",
    "        entity_df[name] = entity_df.index.isin(disease_set)\n",
    "    \n",
    "    # Add a column for potential issues (diseases not in all files)\n",
    "    entity_df['in_all_files'] = entity_df.all(axis=1)\n",
    "    \n",
    "    return entity_df\n",
    "\n",
    "def find_similar_diseases(diseases_list, threshold=0.8):\n",
    "    \"\"\"Find similar disease names using string similarity.\"\"\"\n",
    "    similar_pairs = []\n",
    "    diseases_list = sorted(diseases_list)\n",
    "    \n",
    "    for i, disease1 in enumerate(diseases_list):\n",
    "        for disease2 in diseases_list[i+1:]:\n",
    "            similarity = SequenceMatcher(None, disease1.lower(), disease2.lower()).ratio()\n",
    "            if similarity > threshold and disease1 != disease2:\n",
    "                similar_pairs.append((disease1, disease2, similarity))\n",
    "    \n",
    "    return sorted(similar_pairs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "def apply_disease_mapping(df, mapping_dict):\n",
    "    \"\"\"Apply disease name mapping to a DataFrame.\"\"\"\n",
    "    if 'Disease' in df.columns:\n",
    "        return df.replace({'Disease': mapping_dict})\n",
    "    return df\n",
    "\n",
    "def create_comprehensive_table(dfs, entity_df):\n",
    "    \"\"\"Create a comprehensive table linking diseases with symptoms, descriptions, and precautions.\"\"\"\n",
    "    # Get all unique diseases from the entity resolution table\n",
    "    unique_diseases = entity_df.index.tolist()\n",
    "    \n",
    "    # Create a new DataFrame for the comprehensive table\n",
    "    comprehensive_df = pd.DataFrame(index=unique_diseases)\n",
    "    \n",
    "    # Add disease descriptions if available\n",
    "    if 'symptom_Description' in dfs and 'Disease' in dfs['symptom_Description'].columns:\n",
    "        # Create a dictionary mapping diseases to descriptions\n",
    "        descriptions = dict(zip(dfs['symptom_Description']['Disease'], dfs['symptom_Description']['Description']))\n",
    "        comprehensive_df['Description'] = comprehensive_df.index.map(descriptions)\n",
    "    \n",
    "    # Add precautions if available\n",
    "    if 'symptom_precaution' in dfs and 'Disease' in dfs['symptom_precaution'].columns:\n",
    "        # Create dictionaries for each precaution column\n",
    "        for i in range(1, 5):\n",
    "            col_name = f'Precaution_{i}'\n",
    "            if col_name in dfs['symptom_precaution'].columns:\n",
    "                precautions = dict(zip(dfs['symptom_precaution']['Disease'], dfs['symptom_precaution'][col_name]))\n",
    "                comprehensive_df[col_name] = comprehensive_df.index.map(precautions)\n",
    "    \n",
    "    # Add symptoms if available (from dataset.csv)\n",
    "    if 'dataset' in dfs and 'Disease' in dfs['dataset'].columns:\n",
    "        # Get all symptom columns\n",
    "        symptom_cols = [col for col in dfs['dataset'].columns if 'Symptom_' in col]\n",
    "        \n",
    "        # Create a dictionary mapping diseases to their symptoms\n",
    "        disease_symptoms = {}\n",
    "        for _, row in dfs['dataset'].iterrows():\n",
    "            disease = row['Disease']\n",
    "            symptoms = [row[col] for col in symptom_cols if pd.notna(row[col]) and row[col] != '']\n",
    "            if disease in disease_symptoms:\n",
    "                disease_symptoms[disease].update(symptoms)\n",
    "            else:\n",
    "                disease_symptoms[disease] = set(symptoms)\n",
    "        \n",
    "        # Convert sets to sorted lists\n",
    "        for disease in disease_symptoms:\n",
    "            disease_symptoms[disease] = sorted(list(disease_symptoms[disease]))\n",
    "        \n",
    "        # Add to comprehensive table\n",
    "        comprehensive_df['Symptoms'] = comprehensive_df.index.map(disease_symptoms)\n",
    "    \n",
    "    return comprehensive_df\n",
    "\n",
    "def standardize_disease_names(dfs):\n",
    "    \"\"\"Standardize disease names across all files using a predefined mapping.\"\"\"\n",
    "    # Define name mappings\n",
    "    disease_name_mapping = {\n",
    "        \"Dimorphic hemmorhoids(piles)\": \"Dimorphic Hemorrhoids (Piles)\",\n",
    "        \"Dimorphic hemorrhoids(piles)\": \"Dimorphic Hemorrhoids (Piles)\",\n",
    "        \"Osteoarthristis\": \"Osteoarthritis\",\n",
    "        \"Peptic ulcer diseae\": \"Peptic Ulcer Disease\",\n",
    "        \"(vertigo) Paroymsal  Positional Vertigo\": \"Paroxysmal Positional Vertigo\",\n",
    "        \"Diabetes \": \"Diabetes\"\n",
    "    }\n",
    "    \n",
    "    # Apply mapping to all dataframes\n",
    "    standardized_dfs = {}\n",
    "    for name, df in dfs.items():\n",
    "        if 'Disease' in df.columns:\n",
    "            standardized_dfs[name] = apply_disease_mapping(df, disease_name_mapping)\n",
    "            print(f'Applied disease name mapping to {name}')\n",
    "        else:\n",
    "            standardized_dfs[name] = df\n",
    "    \n",
    "    return standardized_dfs\n",
    "\n",
    "def save_standardized_files(dfs):\n",
    "    \"\"\"Save standardized DataFrames to CSV files.\"\"\"\n",
    "    # Create a directory for standardized files if it doesn't exist\n",
    "    if not os.path.exists('standardized'):\n",
    "        os.makedirs('standardized')\n",
    "    \n",
    "    # Save each DataFrame to a CSV file\n",
    "    for name, df in dfs.items():\n",
    "        file_path = os.path.join('standardized', f'{name}.csv')\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Saved standardized file to {file_path}')\n",
    "\n",
    "def save_comprehensive_table(comprehensive_df):\n",
    "    \"\"\"Save the comprehensive table to a CSV file.\"\"\"\n",
    "    # Create a directory for output files if it doesn't exist\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "    \n",
    "    # Save the comprehensive table\n",
    "    file_path = os.path.join('output', 'comprehensive_disease_data.csv')\n",
    "    comprehensive_df.to_csv(file_path)\n",
    "    print(f'Saved comprehensive table to {file_path}')\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entity resolution process.\"\"\"\n",
    "    print(\"Loading CSV files...\")\n",
    "    dfs = load_csv_files()\n",
    "    \n",
    "    print(\"\\nCreating entity resolution table...\")\n",
    "    entity_df = create_entity_resolution_table(dfs)\n",
    "    \n",
    "    # Display the entity resolution table\n",
    "    print(\"\\nEntity Resolution Table:\")\n",
    "    print(entity_df)\n",
    "    \n",
    "    # Show diseases that are not in all files\n",
    "    missing_diseases = entity_df[~entity_df['in_all_files']]\n",
    "    if len(missing_diseases) > 0:\n",
    "        print(\"\\nDiseases not in all files (potential entity resolution issues):\")\n",
    "        print(missing_diseases)\n",
    "    else:\n",
    "        print(\"\\nAll diseases appear in all files.\")\n",
    "    \n",
    "    # Find similar disease names\n",
    "    all_diseases = entity_df.index.tolist()\n",
    "    similar_diseases = find_similar_diseases(all_diseases)\n",
    "    \n",
    "    if similar_diseases:\n",
    "        print(\"\\nPotential similar disease names that might need standardization:\")\n",
    "        similar_df = pd.DataFrame(similar_diseases, columns=['Disease 1', 'Disease 2', 'Similarity'])\n",
    "        print(similar_df)\n",
    "    else:\n",
    "        print(\"\\nNo similar disease names found.\")\n",
    "    \n",
    "    # Standardize disease names\n",
    "    print(\"\\nStandardizing disease names...\")\n",
    "    standardized_dfs = standardize_disease_names(dfs)\n",
    "    \n",
    "    # Create comprehensive table\n",
    "    print(\"\\nCreating comprehensive table...\")\n",
    "    comprehensive_df = create_comprehensive_table(standardized_dfs, entity_df)\n",
    "    \n",
    "    # Save standardized files\n",
    "    print(\"\\nSaving standardized files...\")\n",
    "    save_standardized_files(standardized_dfs)\n",
    "    \n",
    "    # Save comprehensive table\n",
    "    print(\"\\nSaving comprehensive table...\")\n",
    "    save_comprehensive_table(comprehensive_df)\n",
    "    \n",
    "    print(\"\\nEntity resolution process completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f74524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV files...\n",
      "\n",
      "Checking for missing values...\n",
      "\n",
      "Checking for duplicates...\n",
      "\n",
      "Data cleaning process completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_csv_files():\n",
    "    \"\"\"Load all CSV files in the current directory into a dictionary of DataFrames.\"\"\"\n",
    "    # Get all CSV files in the current directory\n",
    "    csv_files = glob.glob('*.csv')\n",
    "    \n",
    "    # Create a dictionary to store all dataframes\n",
    "    dfs = {}\n",
    "    \n",
    "    # Read each CSV file\n",
    "    for file in csv_files:\n",
    "        # Get the name without extension\n",
    "        name = file.split('.')[0]\n",
    "        # Read the CSV file\n",
    "        dfs[name] = pd.read_csv(file)\n",
    "        print(f'Read {file} successfully')\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "def check_missing_values(dfs):\n",
    "    \"\"\"Check for missing values in all DataFrames.\"\"\"\n",
    "    for name, df in dfs.items():\n",
    "        print(f'\\n{name} - Missing Values:')\n",
    "        missing = df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(missing[missing > 0])\n",
    "        else:\n",
    "            print('No missing values')\n",
    "\n",
    "def check_duplicates(dfs):\n",
    "    \"\"\"Check for duplicate rows in all DataFrames.\"\"\"\n",
    "    for name, df in dfs.items():\n",
    "        duplicates = df.duplicated().sum()\n",
    "        print(f'\\n{name} - Duplicates:')\n",
    "        if duplicates > 0:\n",
    "            print(f'Found {duplicates} duplicate rows')\n",
    "        else:\n",
    "            print('No duplicate rows found')\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"Apply comprehensive cleaning to a DataFrame.\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(f'Missing values before cleaning:\\n{df_clean.isnull().sum()}')\n",
    "    \n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols = df_clean.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    cat_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])\n",
    "    \n",
    "    print(f'\\nMissing values after cleaning:\\n{df_clean.isnull().sum()}')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    duplicates = df_clean.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f'\\nFound {duplicates} duplicate rows')\n",
    "        df_clean = df_clean.drop_duplicates()\n",
    "        print(f'Removed all duplicate rows')\n",
    "    else:\n",
    "        print('\\nNo duplicate rows found')\n",
    "    \n",
    "    # Standardize text data (lowercase for text columns)\n",
    "    for col in cat_cols:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = df_clean[col].str.strip().str.lower()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def clean_all_dataframes(dfs):\n",
    "    \"\"\"Apply cleaning to all DataFrames.\"\"\"\n",
    "    cleaned_dfs = {}\n",
    "    for name, df in dfs.items():\n",
    "        print(f'\\nCleaning {name}...')\n",
    "        cleaned_dfs[name] = clean_dataframe(df)\n",
    "    return cleaned_dfs\n",
    "\n",
    "def save_cleaned_files(dfs):\n",
    "    \"\"\"Save cleaned DataFrames to CSV files.\"\"\"\n",
    "    # Create a directory for cleaned files if it doesn't exist\n",
    "    if not os.path.exists('cleaned'):\n",
    "        os.makedirs('cleaned')\n",
    "    \n",
    "    # Save each DataFrame to a CSV file\n",
    "    for name, df in dfs.items():\n",
    "        file_path = os.path.join('cleaned', f'{name}.csv')\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Saved cleaned file to {file_path}')\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data cleaning process.\"\"\"\n",
    "    print(\"Loading CSV files...\")\n",
    "    dfs = load_csv_files()\n",
    "    \n",
    "    print(\"\\nChecking for missing values...\")\n",
    "    check_missing_values(dfs)\n",
    "    \n",
    "    print(\"\\nChecking for duplicates...\")\n",
    "    check_duplicates(dfs)\n",
    "    \n",
    "    # Ask user if they want to clean all files\n",
    "    clean_all = input(\"\\nDo you want to clean all files? (y/n): \")\n",
    "    if clean_all.lower() == 'y':\n",
    "        print(\"\\nCleaning all files...\")\n",
    "        cleaned_dfs = clean_all_dataframes(dfs)\n",
    "        \n",
    "        # Ask user if they want to save cleaned files\n",
    "        save_files = input(\"\\nDo you want to save cleaned files? (y/n): \")\n",
    "        if save_files.lower() == 'y':\n",
    "            print(\"\\nSaving cleaned files...\")\n",
    "            save_cleaned_files(cleaned_dfs)\n",
    "    \n",
    "    print(\"\\nData cleaning process completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c032958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                             HEALTH DATA PROCESSOR                              \n",
      "================================================================================\n",
      "\n",
      "This script provides options to clean, analyze, and resolve entities in health data CSV files.\n",
      "Choose an option from the menu below:\n",
      "\n",
      "MENU:\n",
      "1. Clean Data\n",
      "2. Analyze Data\n",
      "3. Entity Resolution\n",
      "4. Run All\n",
      "5. Exit\n",
      "\n",
      "================================================================================\n",
      "                                 DATA CLEANING                                  \n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_cleaning'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m         time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m choice == \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     25\u001b[39m     print_header(\u001b[33m\"\u001b[39m\u001b[33mDATA CLEANING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_cleaning\u001b[39;00m\n\u001b[32m     27\u001b[39m     data_cleaning.main()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m choice == \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_cleaning'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def print_header(title):\n",
    "    \"\"\"Print a formatted header.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{title.center(80)}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the health data processing pipeline.\"\"\"\n",
    "    print_header(\"HEALTH DATA PROCESSOR\")\n",
    "    \n",
    "    print(\"This script provides options to clean, analyze, and resolve entities in health data CSV files.\")\n",
    "    print(\"Choose an option from the menu below:\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nMENU:\")\n",
    "        print(\"1. Clean Data\")\n",
    "        print(\"2. Analyze Data\")\n",
    "        print(\"3. Entity Resolution\")\n",
    "        print(\"4. Run All\")\n",
    "        print(\"5. Exit\")\n",
    "        \n",
    "        choice = input(\"\\nEnter your choice (1-5): \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            print_header(\"DATA CLEANING\")\n",
    "            import data_cleaning\n",
    "            data_cleaning.main()\n",
    "        \n",
    "        elif choice == '2':\n",
    "            print_header(\"DATA ANALYSIS\")\n",
    "            import data_analysis\n",
    "            data_analysis.main()\n",
    "        \n",
    "        elif choice == '3':\n",
    "            print_header(\"ENTITY RESOLUTION\")\n",
    "            import entity_resolution\n",
    "            entity_resolution.main()\n",
    "        \n",
    "        elif choice == '4':\n",
    "            print_header(\"RUNNING COMPLETE PIPELINE\")\n",
    "            \n",
    "            print(\"Step 1: Data Cleaning\")\n",
    "            import data_cleaning\n",
    "            data_cleaning.main()\n",
    "            \n",
    "            print(\"\\nStep 2: Entity Resolution\")\n",
    "            import entity_resolution\n",
    "            entity_resolution.main()\n",
    "            \n",
    "            print(\"\\nStep 3: Data Analysis\")\n",
    "            import data_analysis\n",
    "            data_analysis.main()\n",
    "            \n",
    "            print(\"\\nComplete pipeline executed successfully!\")\n",
    "        \n",
    "        elif choice == '5':\n",
    "            print(\"\\nExiting Health Data Processor. Goodbye!\")\n",
    "            sys.exit(0)\n",
    "        \n",
    "        else:\n",
    "            print(\"\\nInvalid choice. Please enter a number between 1 and 5.\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf5eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading comprehensive medical data...\n",
      "Medical data loaded successfully!\n",
      "\n",
      "===== Enhanced Medical Assessment System =====\n",
      "I'll ask 8 targeted questions and collect physical parameters to assess your condition.\n",
      "Type 'quit' at any time to exit.\n",
      "\n",
      "Please describe your symptoms in detail:\n",
      "\n",
      "I need more specific information about your symptoms. Let me ask some questions.\n",
      "\n",
      "Question 1/8: Have you noticed high fever?\n",
      "\n",
      "Question 2/8: I'd like to know if you have high fever?\n",
      "\n",
      "Question 3/8: Have you been experiencing high fever?\n",
      "\n",
      "Question 4/8: To narrow down the diagnosis, are you experiencing high fever?\n",
      "\n",
      "Question 5/8: I'd like to know if you have high fever?\n",
      "\n",
      "Question 6/8: Are you suffering from high fever?\n",
      "\n",
      "Question 7/8: This is important: have you had any high fever?\n",
      "\n",
      "Question 8/8: Do you have any high fever?\n",
      "\n",
      "I'd like to collect some additional physical parameters to improve my assessment.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "Please enter a valid number or 'skip'.\n",
      "\n",
      "===== Medical Assessment =====\n",
      "\n",
      "Based on the information provided, I cannot make a specific diagnosis. The symptoms you've described are too general or may not match any specific condition in our database. Please consult a healthcare professional for a proper evaluation.\n",
      "\n",
      "Would you like another assessment? (yes/no)\n",
      "Thank you for using the Enhanced Medical Assessment System. Take care of your health!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class EnhancedDiagnosisSystem:\n",
    "    \"\"\"An enhanced diagnosis system with physical parameters and dietary recommendations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the enhanced diagnosis system.\"\"\"\n",
    "        print(\"Loading comprehensive medical data...\")\n",
    "        \n",
    "        # Load all CSV files\n",
    "        self.dataset = pd.read_csv('./Datasets/dataset.csv')\n",
    "        self.descriptions = pd.read_csv('./Datasets/symptom_Description.csv')\n",
    "        self.precautions = pd.read_csv('./Datasets/symptom_precaution.csv')\n",
    "        self.severity = pd.read_csv('./Datasets/symptom_severity.csv')\n",
    "        \n",
    "        # Extract all symptoms and create dictionaries\n",
    "        self.all_symptoms = self.extract_all_symptoms()\n",
    "        self.disease_symptom_dict = self.create_disease_symptom_dict()\n",
    "        self.symptom_severity_dict = self.create_symptom_severity_dict()\n",
    "        \n",
    "        # Fixed number of questions\n",
    "        self.num_questions = 8\n",
    "        \n",
    "        # Confirmed and denied symptoms\n",
    "        self.confirmed_symptoms = set()\n",
    "        self.denied_symptoms = set()\n",
    "        \n",
    "        # Physical parameters\n",
    "        self.physical_params = {}\n",
    "        \n",
    "        # Dietary recommendations by disease category\n",
    "        self.diet_recommendations = {\n",
    "            'respiratory': [\n",
    "                \"Consume foods rich in vitamin C like citrus fruits and bell peppers\",\n",
    "                \"Include anti-inflammatory foods like turmeric and ginger\",\n",
    "                \"Stay well-hydrated with water and herbal teas\",\n",
    "                \"Avoid dairy products which may increase mucus production\",\n",
    "                \"Consume warm broths and soups to ease respiratory symptoms\"\n",
    "            ],\n",
    "            'digestive': [\n",
    "                \"Eat smaller, more frequent meals\",\n",
    "                \"Include easily digestible foods like rice, bananas, and toast\",\n",
    "                \"Stay hydrated with clear fluids\",\n",
    "                \"Avoid spicy, fatty, and acidic foods\",\n",
    "                \"Consider probiotic foods like yogurt to support gut health\"\n",
    "            ],\n",
    "            'infectious': [\n",
    "                \"Increase protein intake to support immune function\",\n",
    "                \"Consume vitamin C rich foods to boost immunity\",\n",
    "                \"Stay well-hydrated with water and electrolyte drinks\",\n",
    "                \"Include zinc-rich foods like nuts and seeds\",\n",
    "                \"Eat light, easily digestible meals\"\n",
    "            ],\n",
    "            'cardiovascular': [\n",
    "                \"Follow a low-sodium diet\",\n",
    "                \"Include heart-healthy fats like olive oil and avocados\",\n",
    "                \"Eat plenty of fruits, vegetables, and whole grains\",\n",
    "                \"Limit red meat and processed foods\",\n",
    "                \"Consider omega-3 rich foods like fatty fish\"\n",
    "            ],\n",
    "            'neurological': [\n",
    "                \"Include omega-3 fatty acids from fish or flaxseeds\",\n",
    "                \"Consume antioxidant-rich berries and dark chocolate\",\n",
    "                \"Stay hydrated and limit caffeine\",\n",
    "                \"Include vitamin E rich foods like nuts and seeds\",\n",
    "                \"Consider magnesium-rich foods like leafy greens\"\n",
    "            ],\n",
    "            'dermatological': [\n",
    "                \"Increase omega-3 fatty acids for skin health\",\n",
    "                \"Include zinc-rich foods like nuts and seeds\",\n",
    "                \"Stay well-hydrated to support skin hydration\",\n",
    "                \"Consume vitamin A and E rich foods\",\n",
    "                \"Limit dairy and sugar which may exacerbate skin conditions\"\n",
    "            ],\n",
    "            'general': [\n",
    "                \"Maintain a balanced diet with plenty of fruits and vegetables\",\n",
    "                \"Stay well-hydrated with water throughout the day\",\n",
    "                \"Include lean proteins to support recovery\",\n",
    "                \"Consume whole grains for sustained energy\",\n",
    "                \"Limit processed foods, sugar, and alcohol\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Disease categories\n",
    "        self.disease_categories = {\n",
    "            'respiratory': ['Pneumonia', 'Bronchial Asthma', 'Common Cold', 'Tuberculosis'],\n",
    "            'digestive': ['Gastroenteritis', 'GERD', 'Peptic Ulcer Disease', 'Jaundice'],\n",
    "            'infectious': ['Malaria', 'Dengue', 'Typhoid', 'Chicken Pox', 'Hepatitis A'],\n",
    "            'cardiovascular': ['Hypertension', 'Heart Attack', 'Varicose Veins'],\n",
    "            'neurological': ['Migraine', 'Cervical Spondylosis', 'Paralysis', 'Vertigo'],\n",
    "            'dermatological': ['Acne', 'Psoriasis', 'Impetigo'],\n",
    "            'general': ['Diabetes', 'Hypothyroidism', 'Hyperthyroidism', 'Arthritis']\n",
    "        }\n",
    "        \n",
    "        print(\"Medical data loaded successfully!\")\n",
    "    \n",
    "    def extract_all_symptoms(self):\n",
    "        \"\"\"Extract all unique symptoms from the dataset.\"\"\"\n",
    "        symptom_cols = [col for col in self.dataset.columns if 'Symptom_' in col]\n",
    "        all_symptoms = set()\n",
    "        for col in symptom_cols:\n",
    "            symptoms = self.dataset[col].dropna().unique()\n",
    "            all_symptoms.update([s.strip() for s in symptoms if s.strip()])\n",
    "        return sorted(list(all_symptoms))\n",
    "    \n",
    "    def create_disease_symptom_dict(self):\n",
    "        \"\"\"Create a dictionary mapping diseases to their symptoms.\"\"\"\n",
    "        disease_symptom_dict = {}\n",
    "        symptom_cols = [col for col in self.dataset.columns if 'Symptom_' in col]\n",
    "        \n",
    "        for disease in self.dataset['Disease'].unique():\n",
    "            disease_rows = self.dataset[self.dataset['Disease'] == disease]\n",
    "            symptoms = set()\n",
    "            for _, row in disease_rows.iterrows():\n",
    "                for col in symptom_cols:\n",
    "                    if pd.notna(row[col]) and row[col] != '':\n",
    "                        symptoms.add(row[col].strip())\n",
    "            disease_symptom_dict[disease] = sorted(list(symptoms))\n",
    "        \n",
    "        return disease_symptom_dict\n",
    "    \n",
    "    def create_symptom_severity_dict(self):\n",
    "        \"\"\"Create a dictionary mapping symptoms to their severity.\"\"\"\n",
    "        severity_dict = {}\n",
    "        \n",
    "        # Use symptom_severity.csv if available\n",
    "        if self.severity is not None:\n",
    "            for _, row in self.severity.iterrows():\n",
    "                severity_dict[row['Symptom']] = row['weight']\n",
    "        \n",
    "        # Default severity for symptoms not in the files\n",
    "        for symptom in self.all_symptoms:\n",
    "            if symptom not in severity_dict:\n",
    "                severity_dict[symptom] = 1\n",
    "        \n",
    "        return severity_dict\n",
    "    \n",
    "    def extract_symptoms_from_text(self, text):\n",
    "        \"\"\"Extract symptoms from natural language text.\"\"\"\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        extracted_symptoms = []\n",
    "        for symptom in self.all_symptoms:\n",
    "            clean_symptom = symptom.replace('_', ' ').lower()\n",
    "            if clean_symptom in text:\n",
    "                extracted_symptoms.append(symptom)\n",
    "        return extracted_symptoms\n",
    "    \n",
    "    def get_symptom_severity(self, symptom):\n",
    "        \"\"\"Get the severity of a symptom.\"\"\"\n",
    "        return self.symptom_severity_dict.get(symptom, 1)\n",
    "    \n",
    "    def predict_disease(self):\n",
    "        \"\"\"Predict diseases based on confirmed symptoms with severity weighting.\"\"\"\n",
    "        if not self.confirmed_symptoms:\n",
    "            return None, 0\n",
    "        \n",
    "        # Calculate scores for each disease\n",
    "        disease_scores = {}\n",
    "        for disease, disease_symptoms in self.disease_symptom_dict.items():\n",
    "            matching = self.confirmed_symptoms & set(disease_symptoms)\n",
    "            denied = self.denied_symptoms & set(disease_symptoms)\n",
    "            \n",
    "            if matching:\n",
    "                # Calculate severity-weighted score\n",
    "                matching_severity = sum(self.get_symptom_severity(s) for s in matching)\n",
    "                total_severity = sum(self.get_symptom_severity(s) for s in disease_symptoms)\n",
    "                \n",
    "                # Base score is the proportion of severity-weighted symptoms\n",
    "                score = matching_severity / total_severity if total_severity > 0 else 0\n",
    "                \n",
    "                # Penalize for denied symptoms\n",
    "                if denied:\n",
    "                    denied_severity = sum(self.get_symptom_severity(s) for s in denied)\n",
    "                    penalty = denied_severity / total_severity if total_severity > 0 else 0\n",
    "                    score = max(0, score - penalty)\n",
    "                \n",
    "                # Adjust score based on physical parameters if available\n",
    "                if 'temperature' in self.physical_params:\n",
    "                    temp = self.physical_params['temperature']\n",
    "                    # Increase score for fever-related diseases if temperature is high\n",
    "                    if temp > 38 and any(s in ['fever', 'high_fever'] for s in matching):\n",
    "                        score *= 1.2\n",
    "                \n",
    "                disease_scores[disease] = score\n",
    "        \n",
    "        # Find the disease with the highest score\n",
    "        if disease_scores:\n",
    "            top_disease = max(disease_scores.items(), key=lambda x: x[1])\n",
    "            return top_disease[0], top_disease[1]\n",
    "        \n",
    "        return None, 0\n",
    "    \n",
    "    def get_next_question(self):\n",
    "        \"\"\"Get the next best question to ask based on current symptoms.\"\"\"\n",
    "        # Make a prediction based on current symptoms\n",
    "        predicted_disease, _ = self.predict_disease()\n",
    "        \n",
    "        if not predicted_disease:\n",
    "            # If no prediction yet, ask about common symptoms\n",
    "            common_symptoms = ['fever', 'headache', 'fatigue', 'cough', 'pain', 'nausea', 'vomiting', 'diarrhea']\n",
    "            for symptom_word in common_symptoms:\n",
    "                matching_symptoms = [s for s in self.all_symptoms if symptom_word in s.lower()]\n",
    "                for symptom in matching_symptoms:\n",
    "                    if symptom not in self.confirmed_symptoms and symptom not in self.denied_symptoms:\n",
    "                        return symptom\n",
    "            \n",
    "            # If all common symptoms asked, pick a random unasked symptom\n",
    "            unasked = [s for s in self.all_symptoms if s not in self.confirmed_symptoms and s not in self.denied_symptoms]\n",
    "            if unasked:\n",
    "                return random.choice(unasked)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        # Get symptoms for the predicted disease\n",
    "        disease_symptoms = set(self.disease_symptom_dict[predicted_disease])\n",
    "        \n",
    "        # Find unasked symptoms for this disease\n",
    "        unasked_symptoms = disease_symptoms - self.confirmed_symptoms - self.denied_symptoms\n",
    "        \n",
    "        if unasked_symptoms:\n",
    "            # Sort by severity\n",
    "            sorted_symptoms = sorted(unasked_symptoms, key=self.get_symptom_severity, reverse=True)\n",
    "            return sorted_symptoms[0]\n",
    "        \n",
    "        # If all symptoms for the predicted disease have been asked,\n",
    "        # find symptoms that could differentiate between similar diseases\n",
    "        \n",
    "        # Get diseases with some matching symptoms\n",
    "        candidate_diseases = []\n",
    "        for disease, symptoms in self.disease_symptom_dict.items():\n",
    "            if self.confirmed_symptoms & set(symptoms):\n",
    "                candidate_diseases.append(disease)\n",
    "        \n",
    "        # Get all symptoms for candidate diseases\n",
    "        all_candidate_symptoms = set()\n",
    "        for disease in candidate_diseases:\n",
    "            all_candidate_symptoms.update(self.disease_symptom_dict[disease])\n",
    "        \n",
    "        # Find unasked symptoms\n",
    "        unasked_symptoms = all_candidate_symptoms - self.confirmed_symptoms - self.denied_symptoms\n",
    "        \n",
    "        if unasked_symptoms:\n",
    "            # Sort by how many diseases have this symptom (most discriminative first)\n",
    "            symptom_counts = {}\n",
    "            for symptom in unasked_symptoms:\n",
    "                count = sum(1 for disease in candidate_diseases if symptom in self.disease_symptom_dict[disease])\n",
    "                # Most discriminative are those that appear in about half the diseases\n",
    "                symptom_counts[symptom] = abs(count - len(candidate_diseases) / 2)\n",
    "            \n",
    "            # Get the most discriminative symptom\n",
    "            sorted_symptoms = sorted(symptom_counts.items(), key=lambda x: x[1])\n",
    "            return sorted_symptoms[0][0] if sorted_symptoms else None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def format_question(self, symptom, question_num):\n",
    "        \"\"\"Format a doctor-like question about a symptom.\"\"\"\n",
    "        clean_symptom = symptom.replace('_', ' ').lower()\n",
    "        \n",
    "        # Doctor-like question templates\n",
    "        templates = [\n",
    "            f\"Question {question_num}/8: Have you been experiencing {clean_symptom}?\",\n",
    "            f\"Question {question_num}/8: Are you suffering from {clean_symptom}?\",\n",
    "            f\"Question {question_num}/8: Do you have any {clean_symptom}?\",\n",
    "            f\"Question {question_num}/8: Have you noticed {clean_symptom}?\",\n",
    "            f\"Question {question_num}/8: I'd like to know if you have {clean_symptom}?\"\n",
    "        ]\n",
    "        \n",
    "        # Add follow-up templates for later questions\n",
    "        if question_num > 3:\n",
    "            templates.extend([\n",
    "                f\"Question {question_num}/8: Based on your symptoms, I need to ask: do you have {clean_symptom}?\",\n",
    "                f\"Question {question_num}/8: To narrow down the diagnosis, are you experiencing {clean_symptom}?\",\n",
    "                f\"Question {question_num}/8: This is important: have you had any {clean_symptom}?\"\n",
    "            ])\n",
    "        \n",
    "        return random.choice(templates)\n",
    "    \n",
    "    def collect_physical_parameters(self):\n",
    "        \"\"\"Collect physical parameters from the user.\"\"\"\n",
    "        print(\"\\nI'd like to collect some additional physical parameters to improve my assessment.\")\n",
    "        \n",
    "        # Body temperature\n",
    "        while True:\n",
    "            temp = input(\"What is your body temperature in °C (or 'skip')? > \")\n",
    "            if temp.lower() == 'skip':\n",
    "                break\n",
    "            try:\n",
    "                temp = float(temp)\n",
    "                if 35 <= temp <= 42:  # Reasonable temperature range\n",
    "                    self.physical_params['temperature'] = temp\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a valid temperature between 35°C and 42°C.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number or 'skip'.\")\n",
    "        \n",
    "        # Weight\n",
    "        while True:\n",
    "            weight = input(\"What is your weight in kg (or 'skip')? > \")\n",
    "            if weight.lower() == 'skip':\n",
    "                break\n",
    "            try:\n",
    "                weight = float(weight)\n",
    "                if 20 <= weight <= 250:  # Reasonable weight range\n",
    "                    self.physical_params['weight'] = weight\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a valid weight between 20kg and 250kg.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number or 'skip'.\")\n",
    "        \n",
    "        # Height\n",
    "        while True:\n",
    "            height = input(\"What is your height in cm (or 'skip')? > \")\n",
    "            if height.lower() == 'skip':\n",
    "                break\n",
    "            try:\n",
    "                height = float(height)\n",
    "                if 50 <= height <= 250:  # Reasonable height range\n",
    "                    self.physical_params['height'] = height\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a valid height between 50cm and 250cm.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number or 'skip'.\")\n",
    "        \n",
    "        # Calculate BMI if both weight and height are available\n",
    "        if 'weight' in self.physical_params and 'height' in self.physical_params:\n",
    "            height_m = self.physical_params['height'] / 100  # Convert cm to m\n",
    "            bmi = self.physical_params['weight'] / (height_m * height_m)\n",
    "            self.physical_params['bmi'] = round(bmi, 1)\n",
    "        \n",
    "        # Blood pressure (optional)\n",
    "        bp = input(\"What is your blood pressure (e.g., '120/80' or 'skip')? > \")\n",
    "        if bp.lower() != 'skip':\n",
    "            try:\n",
    "                if '/' in bp:\n",
    "                    systolic, diastolic = map(int, bp.split('/'))\n",
    "                    self.physical_params['blood_pressure'] = f\"{systolic}/{diastolic}\"\n",
    "            except:\n",
    "                pass  # Ignore invalid blood pressure\n",
    "    \n",
    "    def get_disease_category(self, disease):\n",
    "        \"\"\"Get the category of a disease.\"\"\"\n",
    "        for category, diseases in self.disease_categories.items():\n",
    "            if disease in diseases:\n",
    "                return category\n",
    "        return 'general'  # Default category\n",
    "    \n",
    "    def get_dietary_recommendations(self, disease):\n",
    "        \"\"\"Get dietary recommendations for a disease.\"\"\"\n",
    "        category = self.get_disease_category(disease)\n",
    "        return self.diet_recommendations.get(category, self.diet_recommendations['general'])\n",
    "    \n",
    "    def format_diagnosis(self, disease, confidence):\n",
    "        \"\"\"Format the diagnosis result with physical parameters and dietary recommendations.\"\"\"\n",
    "        if not disease:\n",
    "            return \"\\n===== Medical Assessment =====\\n\\nBased on the information provided, I cannot make a specific diagnosis. The symptoms you've described are too general or may not match any specific condition in our database. Please consult a healthcare professional for a proper evaluation.\"\n",
    "        \n",
    "        # Get disease details\n",
    "        description = \"\"\n",
    "        description_row = self.descriptions[self.descriptions['Disease'] == disease]\n",
    "        if not description_row.empty:\n",
    "            description = description_row['Description'].iloc[0]\n",
    "        \n",
    "        # Get precautions\n",
    "        precautions = []\n",
    "        precaution_row = self.precautions[self.precautions['Disease'] == disease]\n",
    "        if not precaution_row.empty:\n",
    "            for i in range(1, 5):\n",
    "                col = f'Precaution_{i}'\n",
    "                if col in self.precautions.columns and pd.notna(precaution_row[col].iloc[0]):\n",
    "                    precautions.append(precaution_row[col].iloc[0])\n",
    "        \n",
    "        # Format the diagnosis\n",
    "        result = f\"\\n===== Medical Assessment =====\\n\\n\"\n",
    "        \n",
    "        # Doctor-like introduction\n",
    "        if confidence > 0.7:\n",
    "            result += f\"Based on your symptoms, you most likely have {disease}.\\n\"\n",
    "        elif confidence > 0.5:\n",
    "            result += f\"Your symptoms suggest {disease}, though further evaluation may be needed.\\n\"\n",
    "        else:\n",
    "            result += f\"Your symptoms are consistent with {disease}, but this is a preliminary assessment only.\\n\"\n",
    "        \n",
    "        result += f\"Diagnostic confidence: {confidence:.2f}\\n\\n\"\n",
    "        \n",
    "        # Physical parameters section\n",
    "        if self.physical_params:\n",
    "            result += \"Physical parameters:\\n\"\n",
    "            if 'temperature' in self.physical_params:\n",
    "                temp = self.physical_params['temperature']\n",
    "                temp_status = \"elevated\" if temp > 37.5 else \"normal\" if temp >= 36.5 else \"below normal\"\n",
    "                result += f\"- Body temperature: {temp}°C ({temp_status})\\n\"\n",
    "            \n",
    "            if 'weight' in self.physical_params:\n",
    "                result += f\"- Weight: {self.physical_params['weight']} kg\\n\"\n",
    "            \n",
    "            if 'height' in self.physical_params:\n",
    "                result += f\"- Height: {self.physical_params['height']} cm\\n\"\n",
    "            \n",
    "            if 'bmi' in self.physical_params:\n",
    "                bmi = self.physical_params['bmi']\n",
    "                bmi_category = \"underweight\" if bmi < 18.5 else \"normal\" if bmi < 25 else \"overweight\" if bmi < 30 else \"obese\"\n",
    "                result += f\"- BMI: {bmi} ({bmi_category})\\n\"\n",
    "            \n",
    "            if 'blood_pressure' in self.physical_params:\n",
    "                bp = self.physical_params['blood_pressure']\n",
    "                systolic, diastolic = map(int, bp.split('/'))\n",
    "                bp_status = \"elevated\" if systolic > 130 or diastolic > 80 else \"normal\"\n",
    "                result += f\"- Blood pressure: {bp} mmHg ({bp_status})\\n\"\n",
    "            \n",
    "            result += \"\\n\"\n",
    "        \n",
    "        if description:\n",
    "            result += f\"About this condition:\\n{description}\\n\\n\"\n",
    "        \n",
    "        # Symptom analysis\n",
    "        result += \"Symptom analysis:\\n\"\n",
    "        for symptom in self.confirmed_symptoms:\n",
    "            severity = self.get_symptom_severity(symptom)\n",
    "            severity_text = \"mild\" if severity <= 3 else \"moderate\" if severity <= 6 else \"severe\"\n",
    "            result += f\"- {symptom.replace('_', ' ')} ({severity_text})\\n\"\n",
    "        \n",
    "        # Dietary recommendations\n",
    "        result += \"\\nDietary recommendations:\\n\"\n",
    "        diet_recs = self.get_dietary_recommendations(disease)\n",
    "        for i, rec in enumerate(diet_recs, 1):\n",
    "            result += f\"{i}. {rec}\\n\"\n",
    "        \n",
    "        # Medical advice\n",
    "        result += \"\\nMedical recommendations:\\n\"\n",
    "        if precautions:\n",
    "            for i, precaution in enumerate(precautions, 1):\n",
    "                result += f\"{i}. {precaution}\\n\"\n",
    "        else:\n",
    "            result += \"1. Rest and stay hydrated\\n\"\n",
    "            result += \"2. Monitor your symptoms\\n\"\n",
    "            result += \"3. Seek medical attention if symptoms worsen\\n\"\n",
    "        \n",
    "        result += \"\\nIMPORTANT: This is an automated assessment and not a medical diagnosis. Please consult a healthcare professional for proper evaluation and treatment.\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_diagnosis(self):\n",
    "        \"\"\"Run the enhanced diagnosis process.\"\"\"\n",
    "        # Reset symptoms and parameters\n",
    "        self.confirmed_symptoms = set()\n",
    "        self.denied_symptoms = set()\n",
    "        self.physical_params = {}\n",
    "        \n",
    "        # Get initial input\n",
    "        print(\"\\nPlease describe your symptoms in detail:\")\n",
    "        initial_input = input(\"> \")\n",
    "        \n",
    "        # Extract initial symptoms\n",
    "        initial_symptoms = self.extract_symptoms_from_text(initial_input)\n",
    "        self.confirmed_symptoms.update(initial_symptoms)\n",
    "        \n",
    "        if initial_symptoms:\n",
    "            print(f\"\\nI've noted these symptoms: {', '.join(s.replace('_', ' ') for s in initial_symptoms)}\")\n",
    "            print(\"Let me ask some additional questions to refine my assessment.\")\n",
    "        else:\n",
    "            print(\"\\nI need more specific information about your symptoms. Let me ask some questions.\")\n",
    "        \n",
    "        # Ask exactly 8 questions\n",
    "        questions_asked = 0\n",
    "        \n",
    "        while questions_asked < self.num_questions:\n",
    "            # Get the next question\n",
    "            next_symptom = self.get_next_question()\n",
    "            \n",
    "            if not next_symptom:\n",
    "                break  # No more relevant questions to ask\n",
    "            \n",
    "            # Ask the question\n",
    "            question = self.format_question(next_symptom, questions_asked + 1)\n",
    "            print(f\"\\n{question}\")\n",
    "            \n",
    "            # Get response with immediate processing\n",
    "            response = input(\"> \")\n",
    "            \n",
    "            # Process response immediately\n",
    "            if response.lower().startswith(('y', 'yes')):\n",
    "                self.confirmed_symptoms.add(next_symptom)\n",
    "            elif response.lower().startswith(('n', 'no')):\n",
    "                self.denied_symptoms.add(next_symptom)\n",
    "            # For other responses, we don't add to either set\n",
    "            \n",
    "            questions_asked += 1\n",
    "        \n",
    "        # Collect physical parameters\n",
    "        self.collect_physical_parameters()\n",
    "        \n",
    "        # Make final prediction\n",
    "        disease, confidence = self.predict_disease()\n",
    "        \n",
    "        # Return diagnosis with dietary recommendations\n",
    "        return self.format_diagnosis(disease, confidence)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the enhanced diagnosis system.\"\"\"\n",
    "    # Initialize the system\n",
    "    system = EnhancedDiagnosisSystem()\n",
    "    \n",
    "    print(\"\\n===== Enhanced Medical Assessment System =====\")\n",
    "    print(\"I'll ask 8 targeted questions and collect physical parameters to assess your condition.\")\n",
    "    print(\"Type 'quit' at any time to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Run the diagnosis\n",
    "            diagnosis = system.run_diagnosis()\n",
    "            print(diagnosis)\n",
    "            \n",
    "            # Ask if the user wants to start a new diagnosis\n",
    "            print(\"\\nWould you like another assessment? (yes/no)\")\n",
    "            response = input(\"> \")\n",
    "            \n",
    "            if response.lower() == 'quit' or not response.lower().startswith(('y', 'yes')):\n",
    "                print(\"Thank you for using the Enhanced Medical Assessment System. Take care of your health!\")\n",
    "                break\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nAssessment interrupted.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Health Predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "Model and components loaded successfully.\n",
      "Neural Health Predictor initialized successfully!\n",
      "\n",
      "===== Neural Network Health Diagnosis System =====\n",
      "Describe your symptoms and I'll predict possible conditions.\n",
      "Type 'quit' to exit.\n",
      "\n",
      "Please describe your symptoms:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\n",
      "===== Neural Network Health Assessment =====\n",
      "\n",
      "Your input: \"chest and knee pain\"\n",
      "\n",
      "Identified symptoms: knee pain\n",
      "\n",
      "Top prediction: Hypertension \n",
      "\n",
      "Other common symptoms:\n",
      "- dizziness\n",
      "- headache\n",
      "- chest pain\n",
      "- lack of concentration\n",
      "- loss of balance\n",
      "\n",
      "Recommended precautions:\n",
      "1. meditation\n",
      "2. salt baths\n",
      "3. reduce stress\n",
      "4. get proper sleep\n",
      "\n",
      "Other possible conditions:\n",
      "- GERD\n",
      "- Osteoarthristis\n",
      "\n",
      "Note: This is not a medical diagnosis. Please consult a healthcare professional.\n",
      "\n",
      "Please describe your symptoms:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "===== Neural Network Health Assessment =====\n",
      "\n",
      "Your input: \"chest pain and knee pain\"\n",
      "\n",
      "Identified symptoms: chest pain, knee pain\n",
      "\n",
      "Top prediction: Osteoarthristis\n",
      "Description:\n",
      "Osteoarthritis is the most common form of arthritis, affecting millions of people worldwide. It occurs when the protective cartilage that cushions the ends of your bones wears down over time.\n",
      "\n",
      "Matched symptoms:\n",
      "- knee pain\n",
      "\n",
      "Other common symptoms:\n",
      "- hip joint pain\n",
      "- swelling joints\n",
      "- painful walking\n",
      "- neck pain\n",
      "- joint pain\n",
      "\n",
      "Recommended precautions:\n",
      "1. acetaminophen\n",
      "2. consult nearest hospital\n",
      "3. follow up\n",
      "4. salt baths\n",
      "\n",
      "Other possible conditions:\n",
      "- GERD\n",
      "- Hypertension \n",
      "\n",
      "Note: This is not a medical diagnosis. Please consult a healthcare professional.\n",
      "\n",
      "Please describe your symptoms:\n",
      "Thank you for using the Neural Network Health Diagnosis System. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "class NeuralHealthPredictor:\n",
    "    \"\"\"A neural network-based health diagnosis predictor with NLP capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='health_nn_model.h5', vectorizer_path='vectorizer.pkl', \n",
    "                 label_encoder_path='label_encoder.pkl', train_model=False):\n",
    "        \"\"\"Initialize the neural health predictor.\"\"\"\n",
    "        print(\"Initializing Neural Health Predictor...\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        self.dataset = pd.read_csv('./Datasets/dataset.csv')\n",
    "        self.descriptions = pd.read_csv('./Datasets/symptom_Description.csv')\n",
    "        self.precautions = pd.read_csv('./Datasets/symptom_precaution.csv')\n",
    "        \n",
    "        # Extract all symptoms\n",
    "        self.all_symptoms = self.extract_all_symptoms()\n",
    "        \n",
    "        # Create disease-symptom dictionary\n",
    "        self.disease_symptom_dict = self.create_disease_symptom_dict()\n",
    "        \n",
    "        # Initialize NLP components\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Model paths\n",
    "        self.model_path = model_path\n",
    "        self.vectorizer_path = vectorizer_path\n",
    "        self.label_encoder_path = label_encoder_path\n",
    "        \n",
    "        # Load or train the model\n",
    "        if train_model or not os.path.exists(model_path):\n",
    "            print(\"Training new neural network model...\")\n",
    "            self.prepare_training_data()\n",
    "            self.train_model()\n",
    "        else:\n",
    "            print(\"Loading pre-trained model...\")\n",
    "            self.load_model()\n",
    "        \n",
    "        print(\"Neural Health Predictor initialized successfully!\")\n",
    "    \n",
    "    def extract_all_symptoms(self):\n",
    "        \"\"\"Extract all unique symptoms from the dataset.\"\"\"\n",
    "        symptom_cols = [col for col in self.dataset.columns if 'Symptom_' in col]\n",
    "        all_symptoms = set()\n",
    "        \n",
    "        for col in symptom_cols:\n",
    "            symptoms = self.dataset[col].dropna().unique()\n",
    "            all_symptoms.update([s.strip() for s in symptoms if s.strip()])\n",
    "        \n",
    "        return sorted(list(all_symptoms))\n",
    "    \n",
    "    def create_disease_symptom_dict(self):\n",
    "        \"\"\"Create a dictionary mapping diseases to their symptoms.\"\"\"\n",
    "        disease_symptom_dict = {}\n",
    "        symptom_cols = [col for col in self.dataset.columns if 'Symptom_' in col]\n",
    "        \n",
    "        for disease in self.dataset['Disease'].unique():\n",
    "            disease_rows = self.dataset[self.dataset['Disease'] == disease]\n",
    "            symptoms = set()\n",
    "            \n",
    "            for _, row in disease_rows.iterrows():\n",
    "                for col in symptom_cols:\n",
    "                    if pd.notna(row[col]) and row[col] != '':\n",
    "                        symptoms.add(row[col].strip())\n",
    "            \n",
    "            disease_symptom_dict[disease] = sorted(list(symptoms))\n",
    "        \n",
    "        return disease_symptom_dict\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess text for NLP.\"\"\"\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stop words and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training data for the neural network.\"\"\"\n",
    "        # Create text descriptions of symptoms for each disease\n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for disease, symptoms in self.disease_symptom_dict.items():\n",
    "            # Create multiple text variations for each disease\n",
    "            for _ in range(5):  # Create 5 variations per disease\n",
    "                # Randomly select 70-100% of symptoms\n",
    "                num_symptoms = max(1, int(np.random.uniform(0.7, 1.0) * len(symptoms)))\n",
    "                selected_symptoms = np.random.choice(symptoms, num_symptoms, replace=False)\n",
    "                \n",
    "                # Create a text description\n",
    "                text = ' '.join([s.replace('_', ' ').lower() for s in selected_symptoms])\n",
    "                \n",
    "                texts.append(text)\n",
    "                labels.append(disease)\n",
    "        \n",
    "        # Create a DataFrame\n",
    "        self.train_df = pd.DataFrame({'text': texts, 'disease': labels})\n",
    "        \n",
    "        # Preprocess texts\n",
    "        self.train_df['processed_text'] = self.train_df['text'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Create vectorizer\n",
    "        self.vectorizer = CountVectorizer(max_features=1000)\n",
    "        X = self.vectorizer.fit_transform(self.train_df['processed_text']).toarray()\n",
    "        \n",
    "        # Encode labels\n",
    "        self.diseases = sorted(list(self.train_df['disease'].unique()))\n",
    "        self.label_to_idx = {disease: i for i, disease in enumerate(self.diseases)}\n",
    "        self.idx_to_label = {i: disease for disease, i in self.label_to_idx.items()}\n",
    "        \n",
    "        y = np.array([self.label_to_idx[disease] for disease in self.train_df['disease']])\n",
    "        y = to_categorical(y, num_classes=len(self.diseases))\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Save vectorizer and label encoder\n",
    "        with open(self.vectorizer_path, 'wb') as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "        \n",
    "        with open(self.label_encoder_path, 'wb') as f:\n",
    "            pickle.dump({'label_to_idx': self.label_to_idx, 'idx_to_label': self.idx_to_label}, f)\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the neural network model.\"\"\"\n",
    "        input_dim = self.X_train.shape[1]\n",
    "        output_dim = len(self.diseases)\n",
    "        \n",
    "        model = Sequential([\n",
    "            Input(shape=(input_dim,)),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(output_dim, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self):\n",
    "        \"\"\"Train the neural network model.\"\"\"\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        # Callbacks\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            self.model_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(self.X_test, self.y_test),\n",
    "            callbacks=[early_stopping, model_checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred_proba = self.model.predict(self.X_test)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        y_true = np.argmax(self.y_test, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save(self.model_path)\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the pre-trained model and components.\"\"\"\n",
    "        try:\n",
    "            self.model = load_model(self.model_path)\n",
    "            \n",
    "            with open(self.vectorizer_path, 'rb') as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "            \n",
    "            with open(self.label_encoder_path, 'rb') as f:\n",
    "                encoders = pickle.load(f)\n",
    "                self.label_to_idx = encoders['label_to_idx']\n",
    "                self.idx_to_label = encoders['idx_to_label']\n",
    "                self.diseases = sorted(list(self.label_to_idx.keys()))\n",
    "            \n",
    "            print(\"Model and components loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Training new model instead...\")\n",
    "            self.prepare_training_data()\n",
    "            self.train_model()\n",
    "    \n",
    "    def predict_from_text(self, text, top_n=3):\n",
    "        \"\"\"Predict diseases from text description using the neural network.\"\"\"\n",
    "        # Preprocess the text\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        \n",
    "        # Vectorize\n",
    "        X = self.vectorizer.transform([processed_text]).toarray()\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_proba = self.model.predict(X)[0]\n",
    "        \n",
    "        # Get top N predictions\n",
    "        top_indices = np.argsort(y_pred_proba)[-top_n:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            disease = self.idx_to_label[idx]\n",
    "            confidence = y_pred_proba[idx]\n",
    "            \n",
    "            # Get disease details\n",
    "            description = \"\"\n",
    "            description_row = self.descriptions[self.descriptions['Disease'] == disease]\n",
    "            if not description_row.empty:\n",
    "                description = description_row['Description'].iloc[0]\n",
    "            \n",
    "            # Get precautions\n",
    "            precautions = []\n",
    "            precaution_row = self.precautions[self.precautions['Disease'] == disease]\n",
    "            if not precaution_row.empty:\n",
    "                for i in range(1, 5):\n",
    "                    col = f'Precaution_{i}'\n",
    "                    if col in self.precautions.columns and pd.notna(precaution_row[col].iloc[0]):\n",
    "                        precautions.append(precaution_row[col].iloc[0])\n",
    "            \n",
    "            # Extract symptoms from text that match this disease\n",
    "            disease_symptoms = set(self.disease_symptom_dict[disease])\n",
    "            matched_symptoms = []\n",
    "            \n",
    "            for symptom in disease_symptoms:\n",
    "                clean_symptom = symptom.replace('_', ' ').lower()\n",
    "                if clean_symptom in text.lower():\n",
    "                    matched_symptoms.append(symptom)\n",
    "            \n",
    "            results.append({\n",
    "                'disease': disease,\n",
    "                'confidence': float(confidence),\n",
    "                'description': description,\n",
    "                'precautions': precautions,\n",
    "                'matched_symptoms': matched_symptoms,\n",
    "                'all_symptoms': list(disease_symptoms)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_symptoms_from_text(self, text):\n",
    "        \"\"\"Extract symptoms from natural language text.\"\"\"\n",
    "        # Convert to lowercase and remove punctuation\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Simple keyword matching for symptoms\n",
    "        extracted_symptoms = []\n",
    "        for symptom in self.all_symptoms:\n",
    "            clean_symptom = symptom.replace('_', ' ').lower()\n",
    "            if clean_symptom in text:\n",
    "                extracted_symptoms.append(symptom)\n",
    "        \n",
    "        return extracted_symptoms\n",
    "    \n",
    "    def format_prediction(self, text, predictions):\n",
    "        \"\"\"Format the prediction results for display.\"\"\"\n",
    "        result = f\"\\n===== Neural Network Health Assessment =====\\n\\n\"\n",
    "        result += f\"Your input: \\\"{text}\\\"\\n\\n\"\n",
    "        \n",
    "        # Extract symptoms\n",
    "        extracted_symptoms = self.extract_symptoms_from_text(text)\n",
    "        if extracted_symptoms:\n",
    "            result += f\"Identified symptoms: {', '.join(s.replace('_', ' ') for s in extracted_symptoms)}\\n\\n\"\n",
    "        else:\n",
    "            result += \"No specific symptoms identified from your input.\\n\\n\"\n",
    "        \n",
    "        # Top prediction\n",
    "        top_prediction = predictions[0]\n",
    "        disease = top_prediction['disease']\n",
    "        confidence = top_prediction['confidence']\n",
    "        \n",
    "        result += f\"Top prediction: {disease}\\n\"\n",
    "        # result += f\"Confidence: {confidence:.2f}\\n\\n\"\n",
    "        \n",
    "        if top_prediction['description']:\n",
    "            result += f\"Description:\\n{top_prediction['description']}\\n\\n\"\n",
    "        \n",
    "        if top_prediction['matched_symptoms']:\n",
    "            result += \"Matched symptoms:\\n\"\n",
    "            for symptom in top_prediction['matched_symptoms']:\n",
    "                result += f\"- {symptom.replace('_', ' ')}\\n\"\n",
    "        \n",
    "        other_symptoms = [s for s in top_prediction['all_symptoms'] if s not in top_prediction['matched_symptoms']]\n",
    "        if other_symptoms:\n",
    "            result += \"\\nOther common symptoms:\\n\"\n",
    "            for symptom in other_symptoms[:5]:  # Show only top 5\n",
    "                result += f\"- {symptom.replace('_', ' ')}\\n\"\n",
    "        \n",
    "        if top_prediction['precautions']:\n",
    "            result += \"\\nRecommended precautions:\\n\"\n",
    "            for i, precaution in enumerate(top_prediction['precautions'], 1):\n",
    "                result += f\"{i}. {precaution}\\n\"\n",
    "        \n",
    "        # Other possible conditions\n",
    "        if len(predictions) > 1:\n",
    "            result += \"\\nOther possible conditions:\\n\"\n",
    "            for pred in predictions[1:]:\n",
    "                result += f\"- {pred['disease']}\\n\"\n",
    "        \n",
    "        result += \"\\nNote: This is not a medical diagnosis. Please consult a healthcare professional.\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def diagnose(self, text):\n",
    "        \"\"\"Process user input and return a diagnosis using the neural network.\"\"\"\n",
    "        predictions = self.predict_from_text(text)\n",
    "        return self.format_prediction(text, predictions)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the neural health predictor.\"\"\"\n",
    "    # Check if model exists\n",
    "    model_exists = os.path.exists('health_nn_model.h5')\n",
    "    \n",
    "    # Initialize the predictor\n",
    "    predictor = NeuralHealthPredictor(train_model=not model_exists)\n",
    "    \n",
    "    print(\"\\n===== Neural Network Health Diagnosis System =====\")\n",
    "    print(\"Describe your symptoms and I'll predict possible conditions.\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        print(\"\\nPlease describe your symptoms:\")\n",
    "        user_input = input(\"> \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Thank you for using the Neural Network Health Diagnosis System. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process the input and display the diagnosis\n",
    "        diagnosis = predictor.diagnose(user_input)\n",
    "        print(diagnosis)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try to import nltk data\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "    except:\n",
    "        print(\"Warning: NLTK data download failed. Some NLP features may not work properly.\")\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab71635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
